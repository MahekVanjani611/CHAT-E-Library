# -*- coding: utf-8 -*-
"""chatbot.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1urvhYAikPZwrOkQ-pKxBLm3HjGJBpYxw
"""

!pip install google.generativeai
!pip install langchain_community
!pip install langchain
!pip install pypdf
!pip install docarray
!pip install chromadb
!pip install google-cloud-aiplatform

pip install google.generativeai

pip install langchain_community

from vertexai.preview.language_models import ChatModel, InputOutputTextPair

import google.generativeai as palm
import os


os.environ['API_KEY']= 'AIzaSyBiKoHcxq1_9IdnMxvt-ZVGpzNQoVbH_Yw'
palm.configure(api_key=os.environ['API_KEY'])

response = palm.generate_text(prompt="Tell me a joke")
print(response.result)

topic = input("Enter topic: ")

prompt = f'''
You are an expert at managing resources in a digital library. You have a deep understanding of the principles of {topic} and are skilled at using a variety of software tools to organize and maintain digital collections. You are also familiar with the latest trends in digital library technology and are always looking for new ways to improve the efficiency and effectiveness of digital library services.

Your task is to list and analyze research papers related to {topic}. Take detailed notes on each paper, focusing on the following aspects:

1. Title and authors of the paper/book.
2. Summary of the research/book.
3. Key findings and conclusions.
4. Technologies and tools discussed.
5. Any recommendations for future research or practice.
And also provide the link to the source don't just hallucinate if you don't know write don't know!

Provide a comprehensive analysis of the collected research papers, highlighting the most significant contributions to the field and any gaps that may need further investigation.
'''

completion = palm.generate_text(
    model="models/text-bison-001",
    prompt=prompt,
    temperature=0,
    max_output_tokens=800,
)

print(completion.result)

def get_completion(prompt, model='models/text-bison-001'):
    messages = [{"role": "user", "content": prompt}]
    completion = palm.generate_text(
        model=model,
        prompt=prompt,
        temperature=0, # this is the degree of randomness of the model's output
        max_output_tokens=800,
    )
    return completion.result

messages =  [
{'role':'system', 'content':'Welcome to the Digital Research Library! Please choose a topic for which you want to explore research papers'},
{'role':'user', 'content':'AI'},
{'role':'assistant', 'content':'Here is the summarization of the topic AI and list of research papers'},
{'role':'user', 'content':'I don\'t know'}  ]

prompt = """
You are an expert mathematician from the Victorian era who likes to talk about the Queen. Give the solution of (82)^2.3, while explaining each step in your quirky way.
"""
print(get_completion(prompt))

endo = palm.chat(messages = "Hello")
endo.last

endo = endo.reply("Yo my name's akr how u doing brother?")
endo.last

endo = endo.reply("I am doing pretty well bro.... what do you do fer livin eh?")
endo.last

endo.messages

endo = palm.chat(messages = "What should i read today?", temperature=1)
endo.last

reply = palm.chat(context="Speak like Shakespeare.", messages='Hello')
print(reply.last)

reply = palm.chat(context="Answer everything with a haiku, following the 5/7/5 rhyme pattern.", messages="How's it going?")
print(reply.last)

pip install langchain

pip install pypdf

from langchain.document_loaders import PyPDFLoader

# Load PDF
loaders = [
    # Duplicate documents on purpose - messy data
    PyPDFLoader("https://arxiv.org/pdf/2401.12187")
]
docs = []
for loader in loaders:
    docs.extend(loader.load())

from langchain.text_splitter import RecursiveCharacterTextSplitter
text_splitter = RecursiveCharacterTextSplitter(
    chunk_size = 1500,
    chunk_overlap = 150
)

splits = text_splitter.split_documents(docs)

len(splits)

from langchain.embeddings import GooglePalmEmbeddings
from langchain.llms import GooglePalm
llm = GooglePalm(google_api_key='AIzaSyBiKoHcxq1_9IdnMxvt-ZVGpzNQoVbH_Yw')
llm.temperature = 0.1

pip install google-cloud-aiplatform

x = 'What is ML?'

close_to_x = 'machine learning'

different_from_x = 'This morning I woke up in San Francisco, and took a walk to the Bay Bridge. It was a good, sunny morning with no fog.'

model = "models/embedding-gecko-001"

# Create an embedding
embedding_x1 = palm.generate_embeddings(model=model, text=x)
embedding_x1

x = 'What is machine learning?'

close_to_x = 'Machine learning'

different_from_x = 'This morning I woke up in San Francisco, and took a walk to the Bay Bridge. It was a good, sunny morning with no fog.'

model = "models/embedding-gecko-001"

# Create an embedding
embedding_x2 = palm.generate_embeddings(model=model, text=x)

import numpy as np
xom = np.dot(embedding_x1['embedding'],embedding_x2['embedding'])
xom

from langchain.vectorstores import Chroma

persist_directory = 'E:\stories\chroma'

!rm -rf ./docs/chroma  # remove old database files if any

pip install chromadb

from langchain.embeddings.google_palm import GooglePalmEmbeddings

# Replace OpenAIEmbeddings with VertexAIEmbeddings
embeddings = GooglePalmEmbeddings(google_api_key='AIzaSyB5xBomrmwEg5Eu4ZUxjuv6nwvxDamG5yE')
# embeddings = VertexAIEmbeddings()

vector_db = Chroma.from_documents(
    documents=splits,
    embedding=embeddings,
    persist_directory=persist_directory
)

print(vector_db._collection.count())

question = "What is reward modelling?"

docs = vector_db.similarity_search(question,k=3)

len(docs)

docs[0].page_content

docs[1].page_content

docs[2].page_content

from langchain.chat_models import ChatGooglePalm
chat = ChatGooglePalm(google_api_key='AIzaSyB5xBomrmwEg5Eu4ZUxjuv6nwvxDamG5yE')

from langchain.chains import RetrievalQA

qa_chain = RetrievalQA.from_chain_type(
    llm,
    retriever=vector_db.as_retriever()
)

question = "What does reward modelling mean?"
docs = vector_db.similarity_search(question,k=3)
result = qa_chain({"query": question})
result["result"]

from langchain.prompts import PromptTemplate

# Build prompt
template = """Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. Use three sentences maximum. Keep the answer as concise as possible. Always say "thanks for asking!" at the end of the answer.
{context}
Question: {question}
Helpful Answer:"""
QA_CHAIN_PROMPT = PromptTemplate.from_template(template)

qa_chain = RetrievalQA.from_chain_type(
    llm,
    retriever=vector_db.as_retriever(),
    return_source_documents=True,
    chain_type_kwargs={"prompt": QA_CHAIN_PROMPT}
)

question = "Who is the author of the paper?"

result = qa_chain({"query": question})

result["result"]

llm.predict("What is the capital of INDIA")

# Build prompt
from langchain.prompts import PromptTemplate
template = """Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. Use three sentences maximum. Keep the answer as concise as possible. Always say "thanks for asking!" at the end of the answer.
{context}
Question: {question}
Helpful Answer:"""
QA_CHAIN_PROMPT = PromptTemplate(input_variables=["context", "question"],template=template,)

# Run chain
from langchain.chains import RetrievalQA
question = "What is llm?"
qa_chain = RetrievalQA.from_chain_type(llm,
                                       retriever=vector_db.as_retriever(),
                                       return_source_documents=True,
                                       chain_type_kwargs={"prompt": QA_CHAIN_PROMPT})


result = qa_chain({"query": question})
result["result"]

from langchain.memory import ConversationBufferMemory
memory = ConversationBufferMemory(
    memory_key="chat_history",
    return_messages=True
)

memory

from langchain.chains import ConversationalRetrievalChain
retriever=vector_db.as_retriever()
qa = ConversationalRetrievalChain.from_llm(
    llm,
    retriever=retriever,
    memory=memory
)

from langchain.embeddings.openai import OpenAIEmbeddings
from langchain.text_splitter import CharacterTextSplitter, RecursiveCharacterTextSplitter
from langchain.vectorstores import DocArrayInMemorySearch
from langchain.document_loaders import TextLoader
from langchain.chains import RetrievalQA,  ConversationalRetrievalChain
from langchain.memory import ConversationBufferMemory
from langchain.chat_models import ChatOpenAI
from langchain.document_loaders import TextLoader
from langchain.document_loaders import PyPDFLoader

def load_db(file, chain_type, k):
    # load documents
    loader = PyPDFLoader(file)
    documents = loader.load()
    # split documents
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=150)
    docs = text_splitter.split_documents(documents)
    # define embedding
    embeddings = GooglePalmEmbeddings(google_api_key='AIzaSyB5xBomrmwEg5Eu4ZUxjuv6nwvxDamG5yE')
    # create vector database from data
    db = DocArrayInMemorySearch.from_documents(docs, embeddings)
    # define retriever
    retriever = db.as_retriever(search_type="similarity", search_kwargs={"k": k})
    # create a chatbot chain. Memory is managed externally.
    qa = ConversationalRetrievalChain.from_llm(
        llm=llm,
        chain_type=chain_type,
        retriever=retriever,
        return_source_documents=True,
        return_generated_question=True,
    )
    return qa

pip install docarray

from langchain.prompts import PromptTemplate
from langchain.chains import RetrievalQA

template = """Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. Use three sentences maximum. Keep the answer as concise as possible. Always say "thanks for asking!" at the end of the answer.
{context}
Question: {question}
Helpful Answer:"""

QA_CHAIN_PROMPT = PromptTemplate(input_variables=["context", "question"], template=template)

# Assuming you have llm and vector_db defined somewhere
# Replace the following with your actual code or adjust as needed

procedo = "Y"
while procedo.upper() != "N":
    question = input("Enter something: ")
    qa_chain = RetrievalQA.from_chain_type(
        llm,
        retriever=vector_db.as_retriever(),
        return_source_documents=True,
        chain_type_kwargs={"prompt": QA_CHAIN_PROMPT}
    )

    result = qa_chain({"context": "Your context here", "query": question})
    print("Answer:", result["result"])
    procedo = input("Do you wish to ask more? (Y/N) ")

"""NOTE TAKING

"""

# Function to handle note-taking
def take_notes(question, answer, source_documents):
    note = {
        "question": question,
        "answer": answer,
        "source_documents": [doc.page_content for doc in source_documents] if source_documents else []
    }
    return note

def main():
    print("Welcome to the Digital Library Assistant!")
    file = input("Please enter the path to your PDF file: ")
    k = int(input("Enter the number of similar documents to retrieve (k): "))

    qa_chain = load_db(file, "stuff", k)

    memory = ConversationBufferMemory(memory_key="chat_history", return_messages=True)
    retriever = qa_chain.retriever
    llm = ChatGooglePalm(google_api_key=os.environ['API_KEY'])

    qa = ConversationalRetrievalChain.from_llm(
        llm=llm,
        retriever=retriever,
        memory=memory
    )

    print("Database loaded successfully. You can start asking questions now.")

    # Store notes
    notes = []

    while True:
        user_input = input("You: ")

        if user_input.lower() in ["exit", "quit"]:
            print("Goodbye! Have a great day!")
            break

        result = qa({"question": user_input})

        # Debugging print to understand the structure of result
        print("Debug - Full Result:", result)

        if "answer" in result:
            answer = result["answer"]
            print("Assistant:", answer)

            # Store the result as a note
            note = take_notes(user_input, answer, result.get("source_documents", []))
            print("Note:", note)
            notes.append(note)
            print("Notes:", notes)
            with open('research_notes.txt', 'w') as file:
              for note in notes:
                file.write(f"Question: {note['question']}\n")
                file.write(f"Answer: {note['answer']}\n")
              for doc in note["source_documents"]:
                file.write(f"Source Document: {doc}\n")
            file.write("\n")
        else:
            print("Unexpected result format. Please check the debug output.")

if __name__ == "__main__":
    main()



